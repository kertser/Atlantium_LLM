# Imports
import os
import logging
from dotenv import load_dotenv

from utils import LLM_utils, RAG_utils, FAISS_utils

#%%

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('rag_system.log')
    ]
)

#%%
load_dotenv()
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"  # numpy issue patch

# Retrieve environment variables
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Validate environment variables
if not all([OPENAI_API_KEY]):
    raise ValueError("One or more environment variables are missing. Please check your .env file.")


#%%

def rag_query(query_text, index, metadata, model, processor, device="cpu", top_k=10):
    """
    Perform a Retrieval-Augmented Generation (RAG) query using FAISS and GPT.
    """
    # Step 1: Query FAISS for context
    results = FAISS_utils.query_with_context(index, metadata, device=device, model=model, processor=processor,
                                             text_query=query_text, top_k=top_k)

    # Step 2: Construct prompt with retrieved context
    if not results or not results[0]:
        logging.error("No results retrieved from FAISS.")
        return "No relevant information found in the document store."

    context = "\n".join([
        res['metadata']['content']
        for res in results[0]
    ])

    # Add a print statement to output the context
    print("Retrieved Context:\n", context)

    prompt = (
        f"Answer the following question based on the context provided:\n\n"
        f"Context:\n{context}\n\n"
        f"Question:\n{query_text}\n\n"
        f"Answer:"
    )

    # Step 3: Generate answer using OpenAI API
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]
    try:
        response = LLM_utils.openai_post_request(
            messages=messages,
            model_name="gpt-4",
            max_tokens=150,
            temperature=0,
            api_key=OPENAI_API_KEY
        )
        if 'choices' in response and response['choices']:
            return response['choices'][0]['message']['content'].strip()
        else:
            logging.error("OpenAI API returned no choices.")
            return "No response generated by the model."
    except Exception as e:
        logging.error(f"Failed to get response from OpenAI API: {str(e)}")
        return "An error occurred while generating the response."


def process_input(input_data, index, metadata, model, processor, device):
    """
    Process input data (text or image) to generate a response.
    """

    def detect_input_type(input_data):
        """
        Detect the type of input data. (Helper function to determine if the input is text or an image.)
        :param input_data: The input data, which can be a string (text) or an Image object.
        :return: 'text' for strings, 'image' for PIL Image objects.
        """
        if input_data is None:
            raise ValueError("Input cannot be None")
        if isinstance(input_data, str):
            return 'text'
        elif isinstance(input_data, Image.Image):  # Assuming PIL Image for image input
            return 'image'
        else:
            raise ValueError("Unsupported input type. Expected a string or a PIL Image object.")

    input_type = detect_input_type(input_data)
    top_k = 10  # Number of top results to retrieve

    if input_type == "text":
        response = rag_query(input_data, index, metadata, device=device, model=model, processor=processor, top_k=top_k)
    elif input_type == "image":
        results = FAISS_utils.query_with_context(index, metadata, model=model, processor=processor,
                                                 image_query=input_data, top_k=top_k)
        context = "\n".join([
            f"{res['metadata']['pdf']} - {res['metadata']['type']}: {res['distance']:.4f}"
            for res in results[0]
        ])
        prompt = (
            f"Analyze the provided image and answer the question based on the context below:\n\n"
            f"Context:\n{context}"
        )
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        response = LLM_utils.openai_post_request(
            messages=messages,
            model_name="gpt-4",
            max_tokens=150,
            temperature=0,
            api_key=OPENAI_API_KEY
        )
        return response['choices'][0]['message']['content'].strip()
    else:
        raise ValueError("Unsupported input type.")

    return response  # Return the generated response


#%%
# Initialize CLIP embedding model:
clip_model, clip_processor, device = LLM_utils.CLIP_init()

#%%

# Load index and metadata from vectorstore
index = FAISS_utils.load_faiss_index("faiss_index.bin")
metadata = FAISS_utils.load_metadata("faiss_metadata.json")

query = "List the numbers of voltage pins in RZ163?"

try:
    response = process_input(query, index, metadata, device=device, model=clip_model,
                             processor=clip_processor)
    print("Response:", response)
except Exception as e:
    print(f"An error occurred during processing: {e}")
